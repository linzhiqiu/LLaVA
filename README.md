# ðŸ’¬ CLIP-FlanT5: Multimodal Large Language Model for Text-to-Image Evaluation

*Evaluating text-to-image faithfulness with VQAScore.*

[[Project Page](https://linzhiqiu.github.io/vqascore/)] [[Code for evaluation](https://todo/)]  [[Data](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/docs/Data.md)] [[Model Zoo](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/docs/MODEL_ZOO.md)]

<!-- **Visual Instruction Tuning** (NeurIPS 2023, **Oral**) [[Paper](https://arxiv.org/abs/2304.08485)]<br>
[Haotian Liu*](https://hliu.cc), [Chunyuan Li*](https://chunyuan.li/), [Qingyang Wu](https://scholar.google.ca/citations?user=HDiw-TsAAAAJ&hl=en/), [Yong Jae Lee](https://pages.cs.wisc.edu/~yongjaelee/) (*Equal Contribution) -->

## Release
- [12/15] ðŸ”¥ We released training code for CLIP-FlanT5 for automatic text-to-image faithfulness evaluation!
<!-- - [10/11] The training data and scripts of LLaVA-1.5 are released [here](https://github.com/linzhiqiu/CLIP-FlanT5#train), and evaluation scripts are released [here](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/docs/Evaluation.md)! -->
<!-- - [10/5] ðŸ”¥ LLaVA-1.5 is out! Achieving SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods like Qwen-VL-Chat that use billion-scale data. Check out the [technical report](https://arxiv.org/abs/2310.03744), and explore the [demo](https://llava.hliu.cc/)! Models are available in [Model Zoo](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/docs/MODEL_ZOO.md). -->


<!-- <a href="https://llava.hliu.cc/"><img src="assets/demo.gif" width="70%"></a> -->

<!-- [![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE)
[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/DATA_LICENSE) -->
**Usage and License Notices**: The data and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna, FlanT5, and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.


## Contents
- [Install](#install)
- [Model Weights](#model-weights)
- [Demo](#Demo)
- [Model Zoo](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/docs/MODEL_ZOO.md)
- [Dataset](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/docs/Data.md)
- [Train](#train)
- [Evaluation](#evaluation)

## Install

We follow LLaVA-1.5 for installation. If you already installed the environment from LLaVA, there is no need to create a new environment. If you are using Windows, do *NOT* proceed, see instructions [here](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/docs/Windows.md).

1. Clone this repository and navigate to LLaVA folder
```bash
git clone https://github.com/linzhiqiu/CLIP-FlanT5.git
cd CLIP-FlanT5
```

2. Install Package (if you do not have llava environment installed already)
```Shell
conda create -n llava python=3.10 -y
conda activate llava
pip install --upgrade pip  # enable PEP 660 support
pip install -e .
```

3. Install additional packages for training cases (if you do not have llava environment installed already)
```
pip install -e ".[train]"
pip install flash-attn --no-build-isolation
```

4. Install huggingface_hub
```
python -m pip install huggingface_hub
```

### Upgrade to latest code base

```Shell
git pull
pip install -e .
```

## CLIP-FlanT5 Weights
Please check out our [Model Zoo](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/docs/MODEL_ZOO.md) for all public CLIP-FlanT5 checkpoints, and the instructions of how to use the weights.

## Train

CLIP-FlanT5 consists of two stages: (1) feature alignment stage: use LLaVA-1.5 558K subset of the LAION-CC-SBU dataset to connect a *frozen pretrained* vision encoder to a *frozen FlanT5*; (2) vqa finetuning stage: use 150K LLaVA-chat data and around 515K VQA data from academic-oriented tasks.

CLIP-FlanT5 is trained on 8 A100 GPUs with 80GB memory. To train on fewer GPUs, you can reduce the `per_device_train_batch_size` and increase the `gradient_accumulation_steps` accordingly. Always keep the global batch size the same: `per_device_train_batch_size` x `gradient_accumulation_steps` x `num_gpus`.

### Hyperparameters
We use a similar set of hyperparameters as LLaVA-1.5 in finetuning.  Both hyperparameters used in pretraining and finetuning are provided below.

1. Pretraining

| Hyperparameter | Global Batch Size | Learning rate | Epochs | Max length | Weight decay |
| --- | ---: | ---: | ---: | ---: | ---: |
| CLIP-FlanT5 | 256 | 1e-2 | 1 | 2048 | 0 |

2. Finetuning

| Hyperparameter | Global Batch Size | Learning rate | Epochs | Max length | Weight decay |
| --- | ---: | ---: | ---: | ---: | ---: |
| CLIP-FlanT5 | 96 | 2e-5 | 1 | 2048 | 0 |

### Download Vicuna checkpoints (automatically)

The base model FlanT5, which is a strong QA model developed by Google, will be downloaded automatically when you run our provided training scripts. No action is needed.

### Pretrain (feature alignment)

You can download the 558K subset of the LAION-CC-SBU dataset with BLIP captions use in the LLaVA-1.5 paper [here](https://huggingface.co/datasets/haotianliu/LLaVA-Pretrain) and unzip/put them under "playground/data/LLaVA-Pretrain". The final folder structure should look like:

```
playground/data/
â”œâ”€â”€ LLaVA-Pretrain
â”‚   â””â”€â”€ blip_laion_cc_sbu_558k.json
â”‚   â””â”€â”€ images
```

Pretrain takes around 5 hours for CLIP-FlanT5-XXL on 8x A100 (80G) using the image resolution of 336px. It takes around 2 hours for LLaVA-v1.5-7B.

Training script with DeepSpeed ZeRO-2: [`clip-flant5-xxl-stage-1.sh`](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/scripts/v1_5/clip-flant5-xxl-stage-1.sh).

- `--mm_projector_type mlp2x_gelu`: the two-layer MLP vision-language connector.
- `--vision_tower openai/clip-vit-large-patch14-336`: CLIP ViT-L/14 336px.

If you are using slurm environment, you can also use the slurm script (by changing the default partition name to your own `#SBATCH --partition={your_own_partition}`) provided in [`clip-flant5-xxl-stage-1.slurm`](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/scripts/v1_5/clip-flant5-xxl-stage-1.slurm).

### Finetune (training for VQA)

1. Prepare data

We flattened the LLaVA-1.5 mixture of data (please download from [llava_v1_5_mix665k_flattened_multi_turn.json](https://huggingface.co/datasets/zhiqiulin/CLIP-FlanT5/blob/main/llava_v1_5_mix665k_flattened_multi_turn.json)), and also download the images from constituting datasets:

- COCO: [train2017](http://images.cocodataset.org/zips/train2017.zip)
- GQA: [images](https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip)
- OCR-VQA: [download script](https://drive.google.com/drive/folders/1_GYPY5UkUy7HIcR0zq3ZCFgeZN7BAfm_?usp=sharing), **because LLaVA-1.5 uses `.jpg` format, please run [fix_ocrvqa.py](fix_ocrvqa.py) after downloading and unzipping**
- TextVQA: [train_val_images](https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip)
- VisualGenome: [part1](https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip), [part2](https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip)

After downloading all of them, organize the data as follows in `./playground/data`,

```
â”œâ”€â”€ coco
â”‚   â””â”€â”€ train2017
â”œâ”€â”€ gqa
â”‚   â””â”€â”€ images
â”œâ”€â”€ ocr_vqa
â”‚   â””â”€â”€ images
â”œâ”€â”€ textvqa
â”‚   â””â”€â”€ train_images
â””â”€â”€ vg
    â”œâ”€â”€ VG_100K
    â””â”€â”€ VG_100K_2
```

2. Start training!

You may download the stage-1 pretrained projectors in [Model Zoo](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/docs/MODEL_ZOO.md).

Stage-2 VQA training takes around 80 hours for CLIP-FlanT5-XXL on 8x A100 (80G), due to the increased resolution to 336px and flattening the multi-turn conversations into single-turn. It takes around 60 hours for CLIP-FlanT5-XL on 8x A100 (40G).

Training script with DeepSpeed ZeRO-3: [`clip-flant5-xxl.sh`](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/scripts/v1_5/clip-flant5-xxl.sh). Optionally, if you use slurm, then you may use [`clip-flant5-xxl.slurm` (be sure to change the default slurm partition)](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/scripts/v1_5/clip-flant5-xxl.slurm) (be sure to change the default slurm partition).

New options to note:

- `--version`: the prompt template / conversational style to be used. In original LLaVA-1.5, stage-1 training uses "plain" and stage-2 training uses "v1". We modify these for FlanT5's encoder-decoder architecture but keep the same system message and roles. In addition, we find that split-text training ([BLIP2-FlanT5 reference](https://arxiv.org/abs/2301.12597)) works better for stage-1 training of CLIP-FlanT5. We recommend using "t5_plain_split_text" for stage-1 training, and "t5_v1" for stage-2 training.
- `--mm_projector_type mlp2x_gelu`: we use the two-layer MLP vision-language connector.
- `--vision_tower openai/clip-vit-large-patch14-336`: CLIP ViT-L/14 336px.
- `--image_aspect_ratio pad`: this pads the non-square images to square, instead of cropping them; it slightly reduces hallucination.
- `--group_by_modality_length True`: this should only be used when your instruction tuning dataset contains both language (e.g. ShareGPT) and multimodal (e.g. LLaVA-Instruct). It makes the training sampler only sample a single modality (either image or language) during training, which we observe to speed up training by ~25%, and does not affect the final outcome.

## Evaluation

In LLaVA-1.5, we evaluate models on a diverse set of 12 benchmarks. To ensure the reproducibility, we evaluate the models with greedy decoding. We do not evaluate using beam search to make the inference process consistent with the chat demo of real-time outputs.

See [Evaluation.md](https://github.com/linzhiqiu/CLIP-FlanT5/blob/main/docs/Evaluation.md).

## Citation

If you find it useful for your research and applications, please cite using this BibTeX:
```bibtex

TODO 

@misc{liu2023improvedllava,
      title={Improved Baselines with Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
      publisher={arXiv:2310.03744},
      year={2023},
}
```

## Acknowledgement
- [LLaVA-1.5](https://github.com/haotianliu/LLaVA/): the codebase we built upon.